{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron, LogisticRegressionCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "import gensim.downloader as api\n",
    "from tqdm import tqdm\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import util.helpers as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Negative Reasons:  9178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Customer Service Issue         2910\n",
       "Late Flight                    1665\n",
       "Can't Tell                     1190\n",
       "Cancelled Flight                847\n",
       "Lost Luggage                    724\n",
       "Bad Flight                      580\n",
       "Flight Booking Problems         529\n",
       "Flight Attendant Complaints     481\n",
       "longlines                       178\n",
       "Damaged Luggage                  74\n",
       "Name: negativereason, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"data/\"\n",
    "\n",
    "intent = pd.read_csv(data_path + \"intent_Tweets.csv\", index_col=0)\n",
    "\n",
    "print(\"Number of Negative Reasons: \", intent.negativereason.count())\n",
    "intent.negativereason.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent = intent[intent['negativereason'].notna()]\n",
    "intent.reset_index(drop=True, inplace=True)\n",
    "num_labels = len(intent.negativereason.unique())\n",
    "\n",
    "rmap = {reason: i for i, reason in enumerate(intent.negativereason.unique())}\n",
    "imap = {i: reason for reason, i in rmap.items()}\n",
    "\n",
    "intent.negativereason = intent.negativereason.apply(lambda x: rmap[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=0.0001, max_df=0.9999, ngram_range=(1,3), max_features=10000)\n",
    "X = vectorizer.fit_transform(intent.text)\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(X)\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, intent.negativereason, train_size=0.8, stratify=intent.negativereason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 0.5615468409586056\n",
      "Precision:\t [0.43421053 0.36796537 0.65486726 0.59499264 0.57894737 0.62773723\n",
      " 0.46153846 0.75757576 0.07692308 0.36363636]\n",
      "Recall: \t [0.28448276 0.35714286 0.66666667 0.69415808 0.31132075 0.59310345\n",
      " 0.3125     0.73964497 0.33333333 0.22222222]\n",
      "F1 scores:\t [0.34375    0.36247335 0.66071429 0.6407613  0.40490798 0.60992908\n",
      " 0.37267081 0.74850299 0.125      0.27586207]\n",
      "Average\n",
      "\tPrecision: 0.5671425701633341\n",
      "\tRecall: 0.5615468409586056\n",
      "\tF1: 0.5580196026576756\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(Perceptron(n_jobs=-1), n_jobs=-1)\n",
    "clf.fit(scaler.transform(xTrain), yTrain)\n",
    "# clf.fit(xTrain, yTrain)\n",
    "\n",
    "yPred = clf.predict(xTest)\n",
    "\n",
    "precisions = precision_score(yTest, yPred, average=None)\n",
    "recalls = recall_score(yTest, yPred, average=None)\n",
    "f1s = f1_score(yTest, yPred, average=None)\n",
    "\n",
    "helper.evaluate_score(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 0.599128540305011\n",
      "Precision:\t [0.52631579 0.41921397 0.66386555 0.56947891 0.55555556 0.73282443\n",
      " 0.58       0.8137931  1.         0.66666667]\n",
      "Recall: \t [0.25862069 0.40336134 0.71171171 0.78865979 0.28301887 0.66206897\n",
      " 0.30208333 0.69822485 0.06666667 0.11111111]\n",
      "F1 scores:\t [0.34682081 0.4111349  0.68695652 0.66138329 0.375      0.69565217\n",
      " 0.39726027 0.75159236 0.125      0.19047619]\n",
      "Average\n",
      "\tPrecision: 0.6049504216536128\n",
      "\tRecall: 0.599128540305011\n",
      "\tF1: 0.5807574713402027\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegressionCV(cv=5, max_iter=5000)\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "yPred = clf.predict(xTest)\n",
    "\n",
    "helper.evaluate_score(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 0.5996732026143791\n",
      "Precision:\t [0.48192771 0.42222222 0.65730337 0.59803922 0.49350649 0.71328671\n",
      " 0.53731343 0.76923077 1.         0.5       ]\n",
      "Recall: \t [0.34482759 0.39915966 0.7027027  0.73367698 0.35849057 0.70344828\n",
      " 0.375      0.71005917 0.2        0.16666667]\n",
      "F1 scores:\t [0.40201005 0.41036717 0.67924528 0.65895062 0.41530055 0.70833333\n",
      " 0.44171779 0.73846154 0.33333333 0.25      ]\n",
      "Average\n",
      "\tPrecision: 0.5956719502388577\n",
      "\tRecall: 0.5996732026143791\n",
      "\tF1: 0.5892884478077615\n"
     ]
    }
   ],
   "source": [
    "clf = LinearSVC()\n",
    "clf.fit(xTrain, yTrain)\n",
    "\n",
    "yPred = clf.predict(xTest)\n",
    "\n",
    "helper.evaluate_score(yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\", Truncation=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_og = pd.concat([intent.text, intent.negativereason], axis=1)\n",
    "\n",
    "data = []\n",
    "for t in data_og.iterrows():\n",
    "    temp = {}\n",
    "    temp[\"text\"] = t[1].text\n",
    "    temp[\"label\"] = t[1].negativereason\n",
    "    temp.update(tokenizer(t[1].text))\n",
    "    data.append(temp)\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=num_labels)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./intent_results\",\n",
    "    learning_rate=2e-5,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer_tuned = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = trainer_tuned.hyperparameter_search(n_trials=10, direction=\"maximize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer_tuned.args, n, v)\n",
    "\n",
    "trainer_tuned.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1836\n",
      "  Batch size = 8\n",
      "The following columns in the test set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = trainer_tuned.predict(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\t 0.35185185185185186\n",
      "Precision:\t [0.         0.         0.33707865 0.35436584 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Recall: \t [0.         0.         0.27607362 0.95697074 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "F1 scores:\t [0.         0.         0.30354132 0.5172093  0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Average\n",
      "\tPrecision: 0.17199030086783854\n",
      "\tRecall: 0.35185185185185186\n",
      "\tF1: 0.21756703347160183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jonny\\USC\\Natural Language Processing\\natalies-little-helper\\nlh\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Jonny\\USC\\Natural Language Processing\\natalies-little-helper\\nlh\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "yTrue = [t[\"label\"] for t in test_dict]\n",
    "yPred = [p.argmax() for p in preds[0]]\n",
    "helper.evaluate_score(yTrue, yPred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
